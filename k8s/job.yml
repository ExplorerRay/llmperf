apiVersion: batch/v1
kind: Job
metadata:
  name: llmperf-job
  namespace: llmperf
spec:
  ttlSecondsAfterFinished: 5
  template:
    spec:
      containers:
        - name: llmperf-container
          image: explorerray/llmperf:config
          # command: [
          #   "python", "token_benchmark_ray.py",
          #   "--model", "$(MODEL_NAME)",
          #   "--llm-api", "openai",
          #   "--results-dir", "/results",
          #   # Default values
          #   # "--timeout", "90",
          #   # "--max-num-completed-requests", "10",
          #   # "--num-concurrent-requests", "10",
          #   # "--mean-input-tokens", "550",
          #   # "--stddev-input-tokens", "150",
          #   # "--mean-output-tokens", "150",
          #   # "--stddev-output-tokens", "80"
          # ]
          # env:
          #   - name: OPENAI_API_KEY
          #     value: "LOCAL_OLLAMA" # Ollama doesn't require an API key, just make it not empty
          #   - name: OPENAI_API_BASE
          #     valueFrom:
          #       configMapKeyRef:
          #         name: llmperf-config
          #         key: openai_api_base
          #   - name: MODEL_NAME
          #     valueFrom:
          #       configMapKeyRef:
          #         name: llmperf-config
          #         key: model_name
          volumeMounts:
            - name: results-volume
              mountPath: /results
            - name: config-volume
              mountPath: /etc/config
              readOnly: true
      restartPolicy: Never
      volumes:
        - name: results-volume
          hostPath:
            path: /mnt/results
            type: DirectoryOrCreate
        - name: config-volume
          configMap:
            name: llmperf-config
